name: run
gpu: 0
seed: 42
dev_run: false

# Data parameters
data:
  task: phenotype
  fold: 1
  matched: true
  timestep: 1.0
  ehr_root: /research/miccai/datasets/mimiciv_multimodal_full/data
  resized_cxr_root: /research/mimic_cxr_resized
  image_meta_path: /hdd/datasets/mimic-cxr-jpg/2.0.0/mimic-cxr-2.0.0-metadata.csv
  pkl_dir: './data_pkls'

retrain_teachers: false
ehr_chkp: teacher_model_checkpoints/phenotype_EHR_seed42.ckpt
cxr_chkp: teacher_model_checkpoints/phenotype_CXR_seed42.ckpt


# Model parameters
model:
  hidden_size: 256
  dropout: 0.2
  
  ehr_modal_distill: true  # flag for performing EHR distillation
  cxr_modal_distill: true  # flag for performing CXR distillation
  
  grad_clip_val: 1.0  # gradient clipping value
  
  ada_loss_weight: true  # flag for performing adaptive distillation loss weight
  ada_kd_weight_type: "power_law"
  ada_kd_alpha: 2.0

ehr_model:
  hidden_size: 256
  n_head: 4
  n_layers: 1
  dropout: 0.2
  criterion: "bce"

cxr_model:
  hidden_size: 256
  dropout: 0.2

# Training parameters
training:
  num_workers: 4
  batch_size: 32
  num_epochs: 100
  patience: 10
  learning_rate: 0.0001
  weight_decay: 0

